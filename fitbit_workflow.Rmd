
Reference : 
https://rpubs.com/eR_ic/regression
```{r}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(tidymodels)
library(car)

fitbit_df <- read.csv('fitbit_data.csv')
head(fitbit_df)

fitbit_features <- fitbit_df %>%
select(-'date')
head(fitbit_features)

str(fitbit_df)
```

# Split Data
```{r}
set.seed(123)
split <- initial_split(fitbit_features, prop=0.9)
train <- training(split)
test <- testing(split)
```

# initial model
```{r}
lm <- linear_reg()
```

# recipe
```{r}
lm_recipe <- recipe(calories ~ AZM_minutes+rmssd+nremhr+entropy+sleep_score+deep_sleep_min+resting_heart_rate+stress_score+o2_avg+o2_lower_bound+o2_upper_bound, data=train) %>%

  step_normalize(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors())

```

# workflow
```{r}
lm_workflow <- workflow() %>%
  add_model(lm) %>%
  add_recipe(lm_recipe)
```

# fit workflow
```{r}
lm_fit <- lm_workflow %>%
  fit(data=train)
```

# Evaluation
```{r}
tidy(lm_fit)
```
rsq=0.67
```{r}
library(yardstick)
lm_aug <- lm_fit %>% 
  augment(test) 

lm_aug %>% 
  metrics(truth = calories, estimate = .pred)
```

```{r}
pred <- test %>% 
  bind_cols(lm_fit %>% 
    predict(new_data = test) %>% 
    rename(predictions = .pred))

pred %>% 
  select(c(calories, predictions)) %>% 
  slice_head(n = 10)
```
```{r}
pred %>% 
  ggplot(mapping = aes(x = calories, y = predictions)) +
  geom_point(size = 1.6, color = "steelblue") +
  geom_smooth(method = "lm", se = F, color = 'magenta') +
  ggtitle("Calories Predictions vs. True") +
  xlab("Actual Labels") +
  ylab("Predicted Labels") +
  theme(plot.title = element_text(hjust = 0.5))
```

Conclusion : LInear regression is not the best model.

# Reduced Model
## importance of variable
This time we only use 'AZM_minutes', 'o2_avg', 'resting_heart_rate', 'nremhr', and 'o2_upper_bound'
```{r}
library(vip)
lm_fit %>%
  extract_fit_parsnip() %>%
  vip(aesthetics = list(fill = "steelblue"))
```
```{r}
reduced_lm <- linear_reg()
```

# Reduced Recipe
```{r}
reduced_recipe <- recipe(calories ~ AZM_minutes+nremhr+resting_heart_rate+stress_score+o2_avg+o2_upper_bound, data=train) %>%

  step_normalize(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors())
```

# Reduced Workflow
```{r}
reduced_workflow <- workflow() %>%
  add_model(reduced_lm) %>%
  add_recipe(reduced_recipe)
```

# Redcued fit workflow
```{r}
reduced_fit <- reduced_workflow %>%
  fit(data=train)
```

# evaluate
```{r}
tidy(reduced_fit)
```
rsq~0.678 does not improve much.
```{r}
reduced_aug <- reduced_fit %>% 
  augment(test) 

reduced_aug %>% 
  metrics(truth = calories, estimate = .pred)
```

```{r}
reduced_pred <- test %>% 
  bind_cols(reduced_fit %>% 
    predict(new_data = test) %>% 
    rename(reduced_pred = .pred))

reduced_pred %>% 
  select(c(calories, reduced_pred)) %>% 
  slice_head(n = 10)
```
```{r}
reduced_pred %>% 
  ggplot(mapping = aes(x = calories, y = reduced_pred)) +
  geom_point(size = 1.6, color = "steelblue") +
  geom_smooth(method = "lm", se = F, color = 'magenta') +
  ggtitle("Reduced Calories Predictions vs. True") +
  xlab("Actual Labels") +
  ylab("Predicted Labels") +
  theme(plot.title = element_text(hjust = 0.5))
```

# LASSO Model
rsq does not improve much
```{r}
# Build a lasso model specification
lasso_lm <-
  # Type
  linear_reg(penalty = 1, mixture = 1) %>% 
  # Engine
  set_engine('glmnet') %>% 
  # Mode
  set_mode('regression')

# Train a lasso regression model
lasso_workflow <- workflow() %>%
  add_model(lasso_lm) %>%
  add_recipe(reduced_recipe)

lasso_fit <- lasso_workflow %>%
  fit(data=train)

# Evaluate the model
lasso_aug <- reduced_fit %>% 
  augment(test) 

reduced_aug %>% 
  metrics(truth = calories, estimate = .pred)
```

# Random Forest
rsq=0.60 is lower 
```{r}
# Build a random forest model specification
rf_lm <- rand_forest() %>% 
  set_engine('randomForest') %>% 
  set_mode('regression')

# Train a random forest model 
rf_workflow <- workflow() %>% 
  add_model(rf_lm) %>%
  add_recipe(lm_recipe)

rf_fit <- rf_workflow %>%
  fit(data=train)

# Evaluate the model
rf_aug <- rf_fit %>% 
  augment(test) 

rf_aug %>% 
  metrics(truth = calories, estimate = .pred)
```

# Gradient Boosting
rsq=0.67
boost_recipe <- recipe(calories ~ AZM_minutes+nremhr+resting_heart_rate+stress_score+o2_avg+o2_upper_bound, data=train) %>%
  step_log(all_numeric_predictors()) %>%
  step_zv(all_numeric_predictors()) %>%
  step_normalize(all_numeric_predictors())
```{r}
boost_lm <- boost_tree() %>% 
  set_engine('xgboost') %>% 
  set_mode('regression')



boost_workflow <- workflow() %>%
  add_model(boost_lm) %>%
  add_recipe(lm_recipe)

boost_fit <- boost_workflow %>%
  fit(data=train)

boost_aug <- boost_fit %>% 
  augment(test) 

boost_aug %>% 
  metrics(truth = calories, estimate = .pred)
  
```

# tuning
```{r}
folds <- vfold_cv(data = train, v = 5, repeats = 1)

tree_grid <- grid_regular(tree_depth(),
                          # Use default ranges if you are not sure
                          learn_rate(range = c(0.01, 0.3),trans = NULL), levels = 5)

tree_grid <- tune_grid(
  object = boost_workflow,
  resamples = folds,
  grid = tree_grid
)
```

```{r}
tree_grid %>% 
  collect_metrics() %>% 
  mutate(tree_depth = factor(tree_depth)) %>% 
  ggplot(mapping = aes(x = learn_rate, y = mean,
                       color = tree_depth)) +
  geom_line(size = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = 'free', nrow = 2)+
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```
```{r}
best_tree <- tree_grid %>% 
  select_best('rmse')

best_tree
```
```{r}
final_wf <- boost_workflow %>% 
  finalize_workflow(best_tree)

final_wf
```


```{r}
final_fit <- final_wf %>% 
  last_fit(split)

final_fit %>% 
  collect_metrics()
```

conclusion : rsq=0.678 is the highest, obtained by LASSO and Reduced Model, these also have the lowest rmse and mae.
